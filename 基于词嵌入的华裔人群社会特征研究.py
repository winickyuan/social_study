# -*- coding: utf-8 -*-
"""基于词嵌入的华裔人群社会特征研究.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O9P8_EoImbTtGwS9s2zyhhUCIkNT-UtV
"""

import os
import time
import numpy as np
from gensim.models import FastText
from gensim.test.utils import datapath
from sklearn import preprocessing

"""# 这里是训练词嵌入模型"""

# 
import time
from gensim.models import FastText
from gensim.test.utils import datapath
os.chdir('/gdrive/My Drive/2019project')
corpus_file = datapath('/gdrive/MyDrive/2019project/America/America_news.cor')  # absolute path to corpus#这里需要语料的绝对路径
model = FastText(size=300, window=3, min_count=20, min_n = 1, max_n = 6, word_ngrams = 1) #只要这里吧min_n设置为1，就能得到subword的词嵌入
time1= time.time()
print('开始读入数据，制作词表中...')
model.build_vocab(corpus_file = corpus_file)  # scan over corpus to build the vocabulary
total_words = model.corpus_total_words  # number of words in the corpus
print('开始训练数据...请耐心等待...')
model.train(corpus_file=corpus_file, total_words=total_words, epochs=10)
time2 = time.time()
timetotal = int(time2-time1)
print('词向量已经训练完成！共用时 %d 秒！'%(timetotal))

"""# 载入本国的词嵌入模型（具体的研究实验从这里开始）"""

timea = time.time()
Model = FastText.load('/gdrive/My Drive/2019project/America/AmericaModel_subword.bin')
timeb = time.time()
print('共用时：', timeb-timea, '秒')

"""# 定义余弦距离"""

import numpy as np
def cos_sim(a,b):
  a_norm = np.linalg.norm(a)
  b_norm = np.linalg.norm(b)
  sim = np.dot(a,b)/(a_norm*b_norm)
  return sim

# 写一个标准化函数
def nmlz(vector):
  normalized_vector = np.squeeze(preprocessing.normalize(vector.reshape(1,-1), norm='l2'))
  return normalized_vector

# 定义一个余弦相似度差函数 
def csd(mainstream, chinese, wordlist):
  CSD_dic_zz = {}
  for x in wordlist:
    CSD_dic_zz[x] = cos_sim(Model[x], mainstream) - cos_sim(Model[x], chinese)
  CSD = sorted(CSD_dic_zz.items(), key=lambda x: x[1], reverse=True)
  return CSD

"""# 得到代表群体的向量"""

# 得到语料库词语的列表，按照出现次数排列
with open('/gdrive/MyDrive/2019project/America/America_news.cor','r') as file_read:
  word_list = file_read.read().split()
from collections import Counter
word_with_frequency = Counter(word_list)
sorted_word_list = sorted(word_with_frequency.items(), key=lambda x: x[1], reverse=True)

"""# 华裔群体向量"""

#读入制作后的的华裔群体人名词表,最后得到华裔群体 chinese 的向量
f = open('/gdrive/MyDrive/2019project/utils_new/美国中人名.txt','r')
a = f.read()
b = a.split()
#得到某个词的词向量
fang = Model[b[0]]
chinese = fang - fang

for i in range(len(b)):
  chinese = chinese + Model[b[i]]
print('向量的长度为')
print(len(chinese))
print(cos_sim(Model['钱氏'], Model['姓钱']))
#把华裔群体的向量平均一下
chinese_avg = chinese/len(b)
# 将向量进行标准化，也就是把l2范数规范为1，有利于后续分析
chinese_avg = np.squeeze(preprocessing.normalize(chinese_avg.reshape(1,-1), norm='l2'))

"""## 主流群体向量"""

#读入主流群体的人名词表，并得到群体向量
k = open('/gdrive/MyDrive/2019project/utils_new/美国美人名.txt','r')
c = k.read()
d = c.split()
bayu = Model[d[0]]
mainStream = bayu - bayu
for j in d:
  mainStream = mainStream + Model[j]
print("主流人群向量长度")
print(len(mainStream))
# 平均一下主流人群体向量
mainStream_avg = mainStream/len(d)
# 将向量进行标准化，也就是把l2范数规范为1，有利于后续分析
mainStream_avg = np.squeeze(preprocessing.normalize(mainStream_avg.reshape(1,-1), norm='l2'))
print('模的长度为')
print(np.linalg.norm(mainStream_avg,ord = 2))
print('标准化前后的相似度为')
cos_sim(mainStream, mainStream_avg)

"""# 实验双极形容词"""

# 在形容词轴上进行试验，使用余弦距离
# 积极词-消极词=积极词轴，然后用余弦相似度计算华裔与积极词轴的距离
CSD_dic1 = {}
for x in range(len(pos_adj)):
  value1 = cos_sim((Model[pos_adj[x]]-Model[neg_adj[x]]), chinese_avg)
  element_x1 = pos_adj[x] + '-' + neg_adj[x]
  CSD_dic1[element_x1] = value1
CSD1 = sorted(CSD_dic1.items(), key=lambda x: x[1], reverse=True)



"""# 接下来是进行实验的第二部分，测试与职业词表的距离"""

# 读入职业词表，得到职业词语列表
with open('/gdrive/MyDrive/2019project/utils_new/职业词表.txt','r') as ff:
  aa = ff.read()

occupations = aa.split()

len(occupations)

# 使用余弦距离来测定华裔的职业倾向
ch_occup_cos = {}
for yy in occupations:
  ch_occup_cos[yy]=cos_sim(chinese_avg, Model[yy])

#按照距离对职业进行排序
ch_occup_cos = sorted(ch_occup_cos.items(), key=lambda x: x[1], reverse=True)
ch_occup_cos[:]

# 然后，使用余弦距离来测定本国主流群体的职业倾向
th_occup_cos = {}
for zz in occupations:
  th_occup_cos[zz]=cos_sim(mainStream_avg, Model[zz])

#按照距离对职业进行排序
th_occup_cos = sorted(th_occup_cos.items(), key=lambda x: x[1], reverse=True)
th_occup_cos[:]

"""# 然后，把职业分成七层，看看华裔和主流人群分别离那个层次的职业比较近，可以进行一个排序"""

# 打开职业词表
with open('/gdrive/MyDrive/2019project/utils_new/职业1非熟练体力劳动者.txt', 'r') as zy1:
  zhiye1 = zy1.read().split()
zhiye1
with open('/gdrive/MyDrive/2019project/utils_new/职业2半熟练体力劳动者.txt', 'r') as zy2:
  zhiye2 = zy2.read().split()
zhiye2
with open('/gdrive/MyDrive/2019project/utils_new/职业3熟练体力劳动者.txt', 'r') as zy3:
  zhiye3 = zy3.read().split()
zhiye3
with open('/gdrive/MyDrive/2019project/utils_new/职业4白领工人.txt', 'r') as zy4:
  zhiye4 = zy4.read().split()
zhiye4
with open('/gdrive/MyDrive/2019project/utils_new/职业5小企业主和经营者.txt', 'r') as zy5:
  zhiye5 = zy5.read().split()
zhiye5
with open('/gdrive/MyDrive/2019project/utils_new/职业6专业人员.txt', 'r') as zy6:
  zhiye6 = zy6.read().split()
zhiye6
with open('/gdrive/MyDrive/2019project/utils_new/职业7产业高层.txt', 'r') as zy7:
  zhiye7 = zy7.read().split()
zhiye7

# 直接写一个函数计算每个列表的向量
def vectorize(one_list):
  v_list = Model[one_list[0]] - Model[one_list[0]]
  for element in one_list:
    v_list = v_list + Model[element]
  v_list = v_list / len(one_list)
  return v_list

"""### 得到各层职业的向量"""

v_zhiye1 = vectorize(zhiye1)
v_zhiye2 = vectorize(zhiye2)
v_zhiye3 = vectorize(zhiye3)
v_zhiye4 = vectorize(zhiye4)
v_zhiye5 = vectorize(zhiye5)
v_zhiye6 = vectorize(zhiye6)
v_zhiye7 = vectorize(zhiye7)

# 计算余弦距离,主流与各层职业的距离
print(cos_sim(mainStream_avg, v_zhiye1), 
      cos_sim(mainStream_avg, v_zhiye2), 
      cos_sim(mainStream_avg, v_zhiye3), 
      cos_sim(mainStream_avg, v_zhiye4), 
      cos_sim(mainStream_avg, v_zhiye5), 
      cos_sim(mainStream_avg, v_zhiye6),
      cos_sim(mainStream_avg, v_zhiye7))

# 计算余弦距离,华裔与各层职业的距离
print(cos_sim(chinese_avg, v_zhiye1), 
      cos_sim(chinese_avg, v_zhiye2), 
      cos_sim(chinese_avg, v_zhiye3), 
      cos_sim(chinese_avg, v_zhiye4), 
      cos_sim(chinese_avg, v_zhiye5), 
      cos_sim(chinese_avg, v_zhiye6),
      cos_sim(chinese_avg, v_zhiye7))

"""## 以上实验可以看出，可以尝试制作一个图标来表示差异

# 接下来进行第三个实验，也就是华裔人群的社会融入情况，从教育、政治、经济、宗教四个方面来考察
"""

# 定义一个新的融入度，用余弦相似度比的方法。
def csr(mainstream, chinese, aspect):
  v1 = cos_sim(chinese, aspect)
  v2 = cos_sim(mainstream, aspect)
  degree = v1 / v2 
  return degree

"""## 政治方面的融入情况"""

# 打开政治词汇表，政治词汇表是选自《现代汉语分类词典》中的政治部分
with open('/gdrive/MyDrive/2019project/utils_new/政治词汇表.txt', 'r') as ll:
  politics = ll.read().split()

# 打印政治词汇的信息看看
print('政治词汇的数个数为',len(politics), '\n', type(politics), '\n',politics[:5])

zhengzhi_avg = vectorize(politics)

cos_sim(mainStream_avg, zhengzhi_avg)

cos_sim(chinese_avg, zhengzhi_avg)

# 华裔的政治融入度为
print('华裔的政治融入度为')
csr(mainStream_avg, chinese_avg, zhengzhi_avg)

"""## 教育方面的融入情况"""

# 打开教育词汇表，词汇表是选自《现代汉语分类词典》中的教育部分
with open('/gdrive/MyDrive/2019project/utils_new/教育词汇表.txt', 'r') as ll:
  education = ll.read().split()
# 打印教育词汇的信息看看
print('词汇的数个数为',len(education), '\n', type(education), '\n',education[:5])

# 得到教育的向量
jiaoyu_avg = vectorize(education)

# 用余弦相似度差试一试，看看哪些词语靠近哪一类人
csd(mainStream_avg, chinese_avg, education)

print(cos_sim(mainStream_avg, jiaoyu_avg))
print(cos_sim(chinese_avg, jiaoyu_avg))

# 计算华裔与本国本土人的在教育方面的融入度
csr(mainStream_avg, chinese_avg, jiaoyu_avg)
# 得到的融入度为0.76，说明与主流人群相比，华裔融入教育的程度为0.76，比政治方面的融入要高

"""## 经贸方面的融入情况"""

# 打开经贸词汇表，词汇表是选自《现代汉语分类词典》
with open('/gdrive/MyDrive/2019project/utils_new/经贸词汇表重制.txt', 'r') as ll:
  economy = ll.read().split()
# 打印经贸词汇的信息看看
print('词汇的个数为',len(economy), '\n', type(economy), '\n',economy[:5])

jingmao_avg = vectorize(economy)

print(cos_sim(mainStream_avg, jingmao_avg))
print(cos_sim(chinese_avg, jingmao_avg))

csd(mainStream_avg, chinese_avg, economy)

# 计算华裔与本国本土人的在经贸方面的融入度
csr(mainStream_avg, chinese_avg, jingmao_avg)
# 得到的融入度为

"""## 宗教方面的融入情况"""

# 打开宗教词汇表，词汇表是选自《现代汉语分类词典》
with open('/gdrive/MyDrive/2019project/utils_new/宗教词汇表0.txt', 'r') as ll:
  religion = ll.read().split()
# 打印宗教词汇的信息看看
print('词汇的个数为',len(religion), '\n', type(religion), '\n',religion[:5])
zongjiao_avg = vectorize(religion)

print(cos_sim(mainStream_avg, zongjiao_avg))
print(cos_sim(chinese_avg, zongjiao_avg))

# 用余弦距离差试一试，看看哪些词语靠近哪一类人
csd(mainStream_avg, chinese_avg, religion)

# 计算华裔与本国本土人的在宗教方面的向量投影差和融入度
csr(mainStream_avg, chinese_avg, zongjiao_avg)
# 得到的融入度为

"""## 第三部分的结果"""

# 得到各个方面的融入程度，并做成列表
zongjiao_csr = csr(mainStream_avg, chinese_avg, zongjiao_avg)
zhengzhi_csr = csr(mainStream_avg, chinese_avg, zhengzhi_avg)
jingmao_csr = csr(mainStream_avg, chinese_avg, jingmao_avg)
jiaoyu_csr = csr(mainStream_avg, chinese_avg, jiaoyu_avg)
csr_list = [('宗教',zongjiao_csr), ('政治',zhengzhi_csr), ('经贸',jingmao_csr), ('教育',jiaoyu_csr)]

csr_list = sorted(csr_list, key=lambda x: x[1], reverse=True)
csr_list
# 最后排序后，得到融入程度从高到低的顺序